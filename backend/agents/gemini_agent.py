from google import genai
from google.genai import types
import os
import json
import sys
import time

# Hack to import from sibling directory if not in path (FastAPI usually handles this, but good for standalone testing)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from services.rag_service import rag_service

class FreshLogicAgent:
    def __init__(self):
        # Initialize Gemini Client
        self.client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))
        
        # Model fallback hierarchy (best to most lenient rate limits)
        # Note: gemini-3-flash is preview only, use 2.5 for production
        self.model_hierarchy = [
            "gemini-2.5-flash",      # Primary - fast, reliable, production-ready
            "gemini-2.0-flash",      # Fallback 1 - older but stable
            "gemini-1.5-flash",      # Fallback 2 - legacy, generous limits
        ]
        self.current_model_index = 0
        self.model = self.model_hierarchy[0]
        self.max_retries = 2  # Retries per model before switching
        self.retry_delay = 1.0  # seconds between retries

        # Define Tools
        self.tools = [
            types.Tool(
                google_search_retrieval=types.GoogleSearchRetrieval(
                    dynamic_retrieval_config=types.DynamicRetrievalConfig(
                        mode=types.DynamicRetrievalConfigMode.MODE_DYNAMIC,
                        dynamic_threshold=0.7,
                    )
                )
            )
        ]

    def analyze_situation(self, telemetry_data: dict, spoilage_risk: dict, user_query: str = None) -> str:
        """
        Analyzes the telemetry + risk data and generates an actionable insight.
        """
        
        # 1. Identify Context for RAG
        # Get crop name from context_data (passed from main.py)
        crop_name = "General Perishables"
        
        # Primary: Check metadata.crop (set by main.py)
        if "metadata" in telemetry_data and "crop" in telemetry_data["metadata"]:
            crop_name = telemetry_data["metadata"]["crop"]
        # Fallback: Check top-level crop_type
        elif "crop_type" in telemetry_data:
            crop_name = telemetry_data["crop_type"]
        
        # Log for debugging
        print(f"üåæ Agent: Processing crop = '{crop_name}'")

        # 2. Retrieve Knowledge (RAG)
        # Search Knowledge Base for "Optimal storage for [Crop]"
        print(f"üß† Agent: Querying Knowledge Base for '{crop_name}'...")
        rag_results = rag_service.query_knowledge_base(f"Optimal conditions storage transport {crop_name}", n_results=3)
        
        rag_text = "No internal knowledge found."
        if rag_results:
            rag_text = ""
            for res in rag_results:
                rag_text += f"- {res['document']} (Confidence: {res['score']:.2f})\n"

        # 3. Construct Prompt with RAG Context
        
        # Handle Telemetry Text with comprehensive route analysis
        if "route_summary" in telemetry_data:
            summary_text = telemetry_data["route_summary"]
            avg_temp = telemetry_data.get("avg_temp", "N/A")
            temp_min = telemetry_data.get("temp_min", avg_temp)
            temp_max = telemetry_data.get("temp_max", avg_temp)
            temp_variance = telemetry_data.get("temp_variance", 0)
            danger_zones = telemetry_data.get("danger_zones", 0)
            danger_hours = telemetry_data.get("danger_hours", 0)
            
            telemetry_text = f"""
            - CROP BEING TRANSPORTED: {crop_name}
            - Route: {summary_text}
            - Distance: {telemetry_data.get('distance_km')} km
            - Transit Duration: {telemetry_data.get('duration_hours')} hours
            
            TEMPERATURE ANALYSIS ALONG ROUTE:
            - Average Temperature: {avg_temp}¬∞C
            - Temperature Range: {temp_min}¬∞C to {temp_max}¬∞C (Variance: {temp_variance}¬∞C)
            - Average Humidity: {telemetry_data.get('avg_humidity')}%
            
            RISK ZONES DETECTED:
            - Number of high-risk checkpoints: {danger_zones}
            - Time in danger zones: {danger_hours} hours
            - Highest risk at waypoint {telemetry_data.get('highest_risk_waypoint', 'N/A')} ({telemetry_data.get('highest_risk_temp', 'N/A')}¬∞C)
            """
        else:
            telemetry_text = f"""
            - CROP BEING TRANSPORTED: {crop_name}
            - Truck ID: {telemetry_data.get('truck_id', 'Unknown')}
            - Current Temp: {telemetry_data.get('sensor_data', {}).get('temperature')}¬∞C
            """

        context = f"""
        You are 'FreshLogic', an AI Chief Agronomist specializing in {crop_name} transport.
        
        SOURCE OF TRUTH (Internal Knowledge Base for {crop_name}):
        {rag_text}
        
        CURRENT TRIP REPORT:
        {telemetry_text}
        
        ML MODEL PREDICTION:
        - Overall Spoilage Risk: {spoilage_risk.get('spoilage_risk', 0) * 100:.1f}% ({spoilage_risk.get('status')})
        - Estimated Shelf Life After Delivery: {spoilage_risk.get('days_remaining', 'N/A')} days
        
        USER QUERY: {user_query or "Provide a comprehensive analysis of this trip."}
        
        INSTRUCTIONS:
        1. You ARE analyzing {crop_name}. Do NOT ask "what crop" - you already know it's {crop_name}.
        2. ANALYZE the temperature variance along the route. High variance (>10¬∞C) is especially dangerous.
        3. CHECK the Internal Knowledge Base. Does ANY temperature along the route violate optimal rules for {crop_name}?
        4. HIGHLIGHT danger zones - where on the route is the crop at highest risk?
        5. Explain WHY the ML predicted {spoilage_risk.get('status')} based on the specific conditions.
        6. Provide 2-3 actionable recommendations (e.g., reefer truck, night transport, alternate route).
        7. Keep response CONCISE - max 150 words. Use bullet points. No fluff.
        8. Format with markdown: Use **bold** for key terms, bullet lists for recommendations.
        """

        # 4. Call Gemini with automatic model fallback
        while self.current_model_index < len(self.model_hierarchy):
            current_model = self.model_hierarchy[self.current_model_index]
            
            for attempt in range(self.max_retries):
                try:
                    print(f"ü§ñ Trying model: {current_model} (attempt {attempt + 1}/{self.max_retries})")
                    response = self.client.models.generate_content(
                        model=current_model,
                        contents=context,
                        config=types.GenerateContentConfig(
                            temperature=0.3 
                        )
                    )
                    # Success! Update current model for future calls
                    self.model = current_model
                    return response.text
                except Exception as e:
                    error_str = str(e)
                    if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str or "quota" in error_str.lower():
                        if attempt < self.max_retries - 1:
                            wait_time = self.retry_delay * (2 ** attempt)
                            print(f"‚ö†Ô∏è Rate limited on {current_model}. Retrying in {wait_time}s...")
                            time.sleep(wait_time)
                            continue
                        else:
                            # Move to next model
                            print(f"‚ùå {current_model} exhausted. Switching to next model...")
                            break
                    else:
                        # Non-rate-limit error, try next model
                        print(f"‚ö†Ô∏è Error with {current_model}: {error_str[:100]}")
                        break
            
            # Move to next model in hierarchy
            self.current_model_index += 1
            if self.current_model_index < len(self.model_hierarchy):
                print(f"üîÑ Falling back to: {self.model_hierarchy[self.current_model_index]}")
        
        # Reset for next request (allow retry from top next time)
        self.current_model_index = 0
        return "‚ö†Ô∏è All models exhausted. Please wait a minute and try again. (Rate limits will reset shortly)"

    def quick_chat(self, message: str, context: dict) -> str:
        """
        Fast chat response using pre-computed context.
        NO external API calls (routing, weather) - just Gemini.
        This is 5-10x faster than full analyze_situation().
        """
        crop_name = context.get("crop_type", "Unknown Crop")
        
        # Build lightweight prompt with cached analysis data
        prompt = f"""You are FreshLogic AI, an expert agronomist assistant.

ANALYSIS CONTEXT (already computed):
- Route: {context.get('origin', 'N/A')} ‚Üí {context.get('destination', 'N/A')}
- Crop: {crop_name}
- Distance: {context.get('distance_km', 'N/A')} km
- Transit Time: {context.get('duration_hours', 'N/A')} hours
- Average Temperature: {context.get('avg_temp', 'N/A')}¬∞C
- Average Humidity: {context.get('avg_humidity', 'N/A')}%
- Spoilage Risk: {context.get('risk_analysis', {}).get('spoilage_risk', 0) * 100:.1f}%
- Status: {context.get('risk_analysis', {}).get('status', 'Unknown')}
- Days Remaining: {context.get('risk_analysis', {}).get('days_remaining', 'N/A')}
- Danger Zones: {context.get('danger_zones', 0)} waypoints above safe temperature

USER QUESTION: {message}

Respond helpfully based on the analysis above. Use bullet points for recommendations.
Be thorough but concise (150-250 words). Use markdown formatting."""

        # Use the fastest available model with retry logic
        for i, model in enumerate(self.model_hierarchy):
            try:
                print(f"üí¨ Chat using model: {model}")
                response = self.client.models.generate_content(
                    model=model,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=0.4,
                        max_output_tokens=800  # Allow complete responses
                    )
                )
                return response.text
            except Exception as e:
                error_str = str(e)
                print(f"‚ö†Ô∏è Chat error with {model}: {error_str[:80]}")
                if i < len(self.model_hierarchy) - 1:
                    print(f"üîÑ Trying next model: {self.model_hierarchy[i+1]}")
                    continue
                return f"I apologize, I'm having trouble responding right now. Please try again in a moment."

agent_service = FreshLogicAgent()
